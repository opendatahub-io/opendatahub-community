{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xskipper -  Extensible Data Skipping Framework for Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate the use of Xskipper Python API.\n",
    "\n",
    "Data skipping can significantly boost the performance of SQL queries by skipping over irrelevant data objects or files based on summary metadata associated with each object.  \n",
    "For each object's column, the summary metadata might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. This metadata is used during query evaluation to skip over objects which have no relevant data.\n",
    "\n",
    "To use this feature, you need to create indexes on one or more columns of the data set. Once indexed, Spark SQL queries can benefit from data skipping. In general, you should index the columns which are queried most often in the WHERE clause.\n",
    "\n",
    "Note that all Spark native data formats are supported, including Parquet, ORC, CSV, JSON and Avro. Data skipping is a performance optimization feature, meaning the use of data skipping does not affect the content of the query results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Indexing a Dataset](#indexing_dataset)\n",
    "- [Index Usage](#using)\n",
    "- [Index Life Cycle](#index_life_cycle)\n",
    "- [Working with Hive Tables](#hive_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we set a JVM wide parameter with a base path to store all the indexes.  \n",
    "Note it's possible to store the metadata on the same storage system as the data but not under the same path.\n",
    "\n",
    "In the following examples we will run spark in local mode and store both data and metadata in the local file system. To run spark in cluster mode update sparkSession to use spark_cluster_url instead of local[*]. To store data and metadata in object storage, set your object store credentials in hconf below.\n",
    "\n",
    "During query time the metdata for the dataset will be looked up in this location\n",
    "\n",
    "For more configuration options, see [Data skipping configuration options](https://xskipper-io.github.io/xskipper/api/configuration/configuration/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "import os\n",
    "\n",
    "# Test os.environ[PYSPARK_SUBMIT_ARGS] includes xskipper in packages\n",
    "# os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use Spark's standalone mode (local), to use cluster mode, update master below to with spark_cluster_url instead of local[*] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "    \n",
    "# Configure some basic spark cluster sizing parameters, if not using local fs - set master with spark_cluster_url\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set('spark.cores.max', 1)\n",
    "spark_conf.set('spark.executor.cores', '1')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('Spark2.4.5-XSKIPPER1.0-Demo') \\\n",
    "    .config(conf = spark_conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'xskipper-core' in spark.sparkContext.getConf().get('spark.jars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: Restart kernel/server in case xskipper jar can not be found in the import below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/example-metadata\n"
     ]
    }
   ],
   "source": [
    "from xskipper import Xskipper\n",
    "\n",
    "# The base location to store all metadata\n",
    "# TODO: change to your metadata location\n",
    "md_base_location = \"/opt/app-root/src/example-metadata\"\n",
    "print(md_base_location)\n",
    "\n",
    "# Configuring the JVM wide parameters \n",
    "# in addition configure the identifier class for IBM Cloud Object Storage\n",
    "conf = dict([\n",
    "            (\"io.xskipper.parquet.mdlocation\", md_base_location),\n",
    "            (\"io.xskipper.parquet.mdlocation.type\", \"EXPLICIT_BASE_PATH_LOCATION\")])\n",
    "Xskipper.setConf(spark, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing a dataset <a id='indexing_dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating a sample Dataset <a id='sample_dataset'></a>\n",
    "\n",
    "First, let's create a sample dataset that will be used throught this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/example\n",
      "+----+---------+---+----------+\n",
      "|temp|city     |vid|dt        |\n",
      "+----+---------+---+----------+\n",
      "|30.0|Jerusalem|b  |2017-07-08|\n",
      "|20.0|Tel-Aviv |a  |2017-07-07|\n",
      "+----+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# TODO: change to your data location\n",
    "dataset_location = \"/opt/app-root/src/example\"\n",
    "print(dataset_location)\n",
    "\n",
    "df_schema = StructType([StructField(\"dt\", StringType(), True), StructField(\"temp\", DoubleType(), True),\\\n",
    "                      StructField(\"city\", StringType(), True), StructField(\"vid\", StringType(), True)]) \n",
    "\n",
    "data = [(\"2017-07-07\", 20.0, \"Tel-Aviv\", \"a\"), (\"2017-07-08\", 30.0, \"Jerusalem\", \"b\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=df_schema) \n",
    "\n",
    "# use partitionBy to make sure we have two objects\n",
    "df.write.partitionBy(\"dt\").mode(\"overwrite\").parquet(dataset_location)\n",
    "\n",
    "# read the dataset back from storage\n",
    "reader = spark.read.format(\"parquet\")\n",
    "df = reader.load(dataset_location)\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing <a id='indexing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create data skipping indexes on a data set, decide which columns to index, then choose an index type for each column. These choices are workload and data dependent. We recommend to select columns which frequently appear in your workload's queries predicates.\n",
    "\n",
    "The following index types are supported out of the box:\n",
    "\n",
    "1. Min/max – stores the minimum and maximum values for a column. Applies to all types except complex types.\n",
    "2. Value list – stores the list of values appearing in a column. Applies to all types except complex types.\n",
    "3. Bloom Filter – stores bloom filter. Applies to ByteType, StringType, LongType, IntegerType, and ShortType.\n",
    "\n",
    " Rule of thumb\n",
    "- Choose value list if the number of distinct values in an object is typically much smaller than the total number of values in that object\n",
    "- Bloom filters are recommended for columns with high cardinality. (otherwise the index can get as big as that column in the data set).\n",
    "\n",
    "Xskipper also enables defining custom (create your own) data skipping indexes and to specify how to apply them during query time. For more details see [here](https://xskipper-io.github.io/xskipper/api/creating-new-plugin/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+\n",
      "|status |new_entries_added|old_entries_removed|\n",
      "+-------+-----------------+-------------------+\n",
      "|SUCCESS|2                |0                  |\n",
      "+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create Xskipper instance for the sample dataset\n",
    "xskipper = Xskipper(spark, dataset_location)\n",
    "\n",
    "# remove index if exists\n",
    "if xskipper.isIndexed():\n",
    "    xskipper.dropIndex()\n",
    "\n",
    "xskipper.indexBuilder() \\\n",
    "        .addMinMaxIndex(\"temp\") \\\n",
    "        .addValueListIndex(\"city\") \\\n",
    "        .addBloomFilterIndex(\"vid\") \\\n",
    "        .build(reader) \\\n",
    "        .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the created index status\n",
    "\n",
    "To view existing dataset's data skipping indexes information and their status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+--------------------+\n",
      "|Data Skipping Index Stats|/opt/app-root/src/example|             Comment|\n",
      "+-------------------------+-------------------------+--------------------+\n",
      "|                   Status|               Up to date|                    |\n",
      "|     Total objects ind...|                        2|                    |\n",
      "|     # Metadata proper...|                         |                    |\n",
      "|        Metadata location|     /opt/app-root/src...|                    |\n",
      "|      # Index information|                         |                    |\n",
      "|             # Index type|                  Columns|              Params|\n",
      "|                   minmax|                     temp|                    |\n",
      "|                valuelist|                     city|                    |\n",
      "|              bloomfilter|                      vid|io.xskipper.index...|\n",
      "+-------------------------+-------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xskipper.describeIndex(reader).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Indexed datasets\n",
    "\n",
    "To view all indexed dataset under the current base location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "+----------------------------------+----------------------------------+-------------+\n",
      "|Dataset                           |Index type                        |Index columns|\n",
      "+----------------------------------+----------------------------------+-------------+\n",
      "|# Metadatastore Manager parameters|                                  |             |\n",
      "|Metadata base path                |/opt/app-root/src/example-metadata|             |\n",
      "|/opt/app-root/src/example         |minmax                            |temp         |\n",
      "|                                  |valuelist                         |city         |\n",
      "|                                  |bloomfilter                       |vid          |\n",
      "+----------------------------------+----------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(xskipper.isIndexed())\n",
    "Xskipper.listIndexes(spark).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Data Skipping Indexes<a id='using'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable/Disable Xskipper<a id='enable_disable'></a>\n",
    "\n",
    "Xskipper provides APIs to enable or disable index usage with Spark.\n",
    "\n",
    "When \"enable\", Xskipper optimization rules become visible to the Apache Spark optimizer and will be used in query optimization and execution.\\\n",
    "When \"disable', Xskipper optimization rules no longer apply during query optimization. Note that disabling Xskipper has no impact on created indexes as they remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable Xskipper\n",
    "Xskipper.enable(spark)\n",
    "\n",
    "# Disable Xskipper\n",
    "Xskipper.disable(spark)\n",
    "\n",
    "# You can use the following to check whether the Xskipper is enabled\n",
    "if not Xskipper.isEnabled(spark):\n",
    "    Xskipper.enable(spark)\n",
    "    \n",
    "Xskipper.isEnabled(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Queries<a id='run_queries_dataset'></a>\n",
    "\n",
    "Once Xskipper has been enabled you can continue running queries (using either SQL or DataFrame API) regularly and enjoy data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Dataset and creating a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reader.load(dataset_location)\n",
    "df.createOrReplaceTempView(\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query using Min/max index\n",
    "Min/max index filters out 1 dataset object in which it's temp column value is >= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+----------+\n",
      "|temp|    city|vid|        dt|\n",
      "+----+--------+---+----------+\n",
      "|20.0|Tel-Aviv|  a|2017-07-07|\n",
      "+----+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from sample where temp < 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting query skipping stats\n",
    "You can inspect the data skipping statistics for the latest query using the following API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+------------+-----------+----------+\n",
      "|status |isSkippable|skipped_Bytes|skipped_Objs|total_Bytes|total_Objs|\n",
      "+-------+-----------+-------------+------------+-----------+----------+\n",
      "|SUCCESS|true       |869          |1           |1729       |2         |\n",
      "+-------+-----------+-------------+------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xskipper.getLatestQueryAggregatedStats(spark).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The above returns the accumulated data skipping statistics for all of the datasets which were involved in the query\n",
    "\n",
    "If you want to inspect the stats for a specific dataset use the following api call on the dataset's Xskipper instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper.getLatestQueryStats().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clearing the stats before running the next query\n",
    "\n",
    "The data skipping stats are accumulated stats of all dataset readings since the last time `clearStats` or `reset` was called.\\\n",
    "Here we clear the stats after each query to get the data skipping stats for each query separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xskipper.clearStats(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query using Value list index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from sample where city IN ('Jerusalem', 'Ramat-Gan')\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting query stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xskipper.getLatestQueryAggregatedStats(spark).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clearing the stats before running the next query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xskipper.clearStats(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Query using Bloom filter index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from sample where vid = 'a'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting query stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xskipper.getLatestQueryAggregatedStats(spark).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clearing the stats before running the next query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xskipper.clearStats(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Life Cycle<a id='index_life_cycle'></a>\n",
    "\n",
    "The following operations can be used in order to maintain the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh Index\n",
    "\n",
    "Overtime the index might get stale in case new files were added/removed/modified in the dataset.\\\n",
    "In order to bring the index up-to-date you can call the refresh operation which will index the new/modified files and remove obsolete metadata.\n",
    "\n",
    "Note: The index will still be useful for files which didn't change since the last indexing time even without refreshing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new file to the dataset to simulate changes in the dataset\n",
    "update_data = [(\"2017-07-09\", 25.0, \"Beer-Sheva\", \"c\")]\n",
    "\n",
    "update_df = spark.createDataFrame(update_data, schema=df_schema) \n",
    "\n",
    "# append to the existing dataset\n",
    "update_df.write.partitionBy(\"dt\").mode(\"append\").parquet(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting index status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper.describeIndex(reader).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status is out of data as there is one new object which is not yet indexed. Let's call the refresh operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper.refreshIndex(reader).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting index status following the refresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper.describeIndex(reader).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Index\n",
    "\n",
    "In order to drop the index use the following API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper.dropIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Hive table<a id='hive_tables'></a>\n",
    "\n",
    "Xskipper also supports skipping over hive tables.\n",
    "\n",
    "The API for working with hive tables is similar to the API presented above with 2 major differences:\n",
    "1. The `uri` used in the Xskipper constructor is the table identifier in the form: `<db>.<table>`.\n",
    "2. The API calls do not require a `DataFrameReader`.\n",
    "\n",
    "For more info regarding the API see [here]()\n",
    "\n",
    "The metadata location for a hive table is resolved according to the following:\n",
    "1. If the table contains the parameter `io.xskipper.parquet.mdlocation` the value will be used as the metadata location\n",
    "2. Else, xskipper will look up the parameter `io.xskipper.parquet.mdlocation` in the table's database and will used it as the base metadata location for all tables.\n",
    "\n",
    "Note: During indexing the index location parameter can be automatically added to the table properties if the xskipper instance is configured accordingly.  \n",
    "For more info regarding the metadata location configuration see [here]().\n",
    "\n",
    "In this example we will set the base location in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the base metadata location in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_db_ddl = (\"ALTER DATABASE default SET DBPROPERTIES ('io.xskipper.parquet.mdlocation'='{0}')\").format(md_base_location)\n",
    "spark.sql(alter_db_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sample Hive Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_ddl = \"\"\"CREATE TABLE IF NOT EXISTS tbl ( \\\n",
    "temp Double,\n",
    "city String,\n",
    "vid String,\n",
    "dt String\n",
    ")\n",
    "USING PARQUET\n",
    "PARTITIONED BY (dt)\n",
    "LOCATION '{0}'\"\"\".format(dataset_location)\n",
    "spark.sql(create_table_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the table partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE tbl RECOVER PARTITIONS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify the table was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show(10, False)\n",
    "spark.sql(\"show partitions tbl\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing a Hive Table\n",
    "\n",
    "notice we use `default.sample` as the uri in the Xskipper constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Xskipper instance for the sample Hive Table\n",
    "xskipper_hive = Xskipper(spark, 'default.tbl')\n",
    "\n",
    "# remove index if exists\n",
    "if xskipper_hive.isIndexed():\n",
    "    xskipper_hive.dropIndex()\n",
    "\n",
    "xskipper_hive.indexBuilder() \\\n",
    "        .addMinMaxIndex(\"temp\") \\\n",
    "        .addValueListIndex(\"city\") \\\n",
    "        .addBloomFilterIndex(\"vid\") \\\n",
    "        .build() \\\n",
    "        .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the created index status\n",
    "\n",
    "The following code shows how a user can view current index status to check which indexes exist on the dataset and whether the index is up-to-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper_hive.describeIndex().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable/Disable Xskipper\n",
    "\n",
    "Make sure Xskipper is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following to check whether the Xskipper is enabled\n",
    "if not Xskipper.isEnabled(spark):\n",
    "    Xskipper.enable(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Queries\n",
    "\n",
    "Once Xskipper has been enabled you can continue running queries (using either SQL or DataFrame API) regularly and enjoy data skipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query using Min/max index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from tbl where temp < 30\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting query skipping stats\n",
    "You can inspect the data skipping statistics for the latest query using the following API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xskipper.getLatestQueryAggregatedStats(spark).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Life Cycle\n",
    "\n",
    "The following operations can be used in order to maintain the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh Index on Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper_hive.refreshIndex().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Index\n",
    "\n",
    "In order to drop the index use the following API call:\n",
    "(Dropping the index will also remove the index location from the table properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xskipper_hive.dropIndex()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
